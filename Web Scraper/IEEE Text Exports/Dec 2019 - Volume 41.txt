"Table of contents," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. C1-C1, Dec. 2019.
doi: 10.1109/TPAMI.2019.2946068
Abstract: Presents the table of contents for this issue of this publication.
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8890762&isnumber=8890757

"[Front inside cover]," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. C2-C2, Dec. 2019.
doi: 10.1109/TPAMI.2019.2946069
Abstract: Provides a listing of current staff, committee members and society officers.
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8890759&isnumber=8890757

F. Tian et al., "Active Camera Relocalization from a Single Reference Image without Hand-Eye Calibration," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 2791-2806, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2870646
Abstract: This paper studies active relocalization of 6D camera pose from a single reference image, a new and challenging problem in computer vision and robotics. Straightforward active camera relocalization (ACR) is a tricky and expensive task that requires elaborate hand-eye calibration on precision robotic platforms. In this paper, we show that high-quality camera relocalization can be achieved in an active and much easier way. We propose a hand-eye calibration free approach to actively relocating the camera to the same 6D pose that produces the input reference image. We theoretically prove that, given bounded unknown hand-eye pose displacement, this approach is able to rapidly reduce both 3D relative rotational and translational pose between current camera and the reference one to an identical matrix and a zero vector, respectively. Based on these findings, we develop an effective ACR algorithm with fast convergence rate, reliable accuracy and robustness. Extensive experiments validate the effectiveness and feasibility of our approach on both laboratory tests and challenging real-world applications in fine-grained change monitoring of cultural heritages.
keywords: {calibration;cameras;image sensors;mobile robots;pose estimation;robot vision;single reference image;computer vision;straightforward active camera relocalization;tricky task;elaborate hand-eye calibration;precision robotic platforms;high-quality camera relocalization;hand-eye calibration free approach;input reference image;unknown hand-eye;effective ACR algorithm;Cameras;Calibration;Three-dimensional displays;Simultaneous localization and mapping;Robot vision systems;Monitoring;Cultural differences;Photography;Active camera relocalization (ACR);6D camera pose;hand-eye calibration free;computational rephotography;fine-grained change monitoring;cultural heritage;preventive conservation},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8466599&isnumber=8890757

M. Diaz, M. A. Ferrer and J. J. Quintana, "Anthropomorphic Features for On-Line Signatures," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 2807-2819, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2869163
Abstract: Many features have been proposed in on-line signature verification. Generally, these features rely on the position of the on-line signature samples and their dynamic properties, as recorded by a tablet. This paper proposes a novel feature space to describe efficiently on-line signatures. Since producing a signature requires a skeletal arm system and its associated muscles, the new feature space is based on characterizing the movement of the shoulder, the elbow and the wrist joints when signing. As this motion is not directly obtained from a digital tablet, the new features are calculated by means of a virtual skeletal arm (VSA) model, which simulates the architecture of a real arm and forearm. Specifically, the VSA motion is described by its 3D joint position and its joint angles. These anthropomorphic features are worked out from both pen position and orientation through the VSA forward and direct kinematic model. The anthropomorphic features' robustness is proved by achieving state-of-the-art performance with several verifiers and multiple benchmarks on third party signature databases, which were collected with different devices and in different languages and scripts.
keywords: {feature extraction;handwriting recognition;image motion analysis;muscle;stereo image processing;anthropomorphic features;on-line signature verification;digital tablet;virtual skeletal arm model;VSA motion;3D joint position;pen position;pen orientation;VSA direct kinematic model;VSA forward kinematic model;handwritten signature;Joints;Feature extraction;Bones;Elbow;Shoulder;Manipulators;Wrist;On-line signature verification;anthropomorphic features;biometrics;performance evaluation;virtual skeletal arm (VSA)},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8457532&isnumber=8890757

B. Yang, S. Rosa, A. Markham, N. Trigoni and H. Wen, "Dense 3D Object Reconstruction from a Single Depth View," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 2820-2834, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2868195
Abstract: In this paper, we propose a novel approach, 3D-RecGAN++, which reconstructs the complete 3D structure of a given object from a single arbitrary depth view using generative adversarial networks. Unlike existing work which typically requires multiple views of the same object or class labels to recover the full 3D geometry, the proposed 3D-RecGAN++ only takes the voxel grid representation of a depth view of the object as input, and is able to generate the complete 3D occupancy grid with a high resolution of 2563 by recovering the occluded/missing regions. The key idea is to combine the generative capabilities of 3D encoder-decoder and the conditional adversarial networks framework, to infer accurate and fine-grained 3D structures of objects in high-dimensional voxel space. Extensive experiments on large synthetic datasets and real-world Kinect datasets show that the proposed 3D-RecGAN++ significantly outperforms the state of the art in single view 3D object reconstruction, and is able to reconstruct unseen types of objects.
keywords: {computational geometry;image reconstruction;image representation;learning (artificial intelligence);neural nets;object detection;shape recognition;solid modelling;stereo image processing;single depth view;single arbitrary depth view;generative adversarial networks;voxel grid representation;3D occupancy grid;3D encoder-decoder;high-dimensional voxel space;dense 3D object reconstruction;conditional adversarial networks;single view 3D object reconstruction;3D-RecGAN++;Kinect datasets;Three-dimensional displays;Image reconstruction;Solid modeling;Periodic structures;Generative adversarial networks;Task analysis;3D Reconstruction;shape completion;shape inpainting;single depth view;adversarial learning;conditional GAN},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8453803&isnumber=8890757

C. F. Benitez-Quiroz, R. Srinivasan and A. M. Martinez, "Discriminant Functional Learning of Color Features for the Recognition of Facial Action Units and Their Intensities," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 2835-2845, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2868952
Abstract: Color is a fundamental image feature of facial expressions. For example, when we furrow our eyebrows in anger, blood rushes in, turning some face areas red; or when one goes white in fear as a result of the drainage of blood from the face. Surprisingly, these image properties have not been exploited to recognize the facial action units (AUs) associated with these expressions. Herein, we present the first system to do recognition of AUs and their intensities using these functional color changes. These color features are shown to be robust to changes in identity, gender, race, ethnicity, and skin color. Specifically, we identify the chromaticity changes defining the transition of an AU from inactive to active and use an innovative Gabor transform-based algorithm to gain invariance to the timing of these changes. Because these image changes are given by functions rather than vectors, we use functional classifiers to identify the most discriminant color features of an AU and its intensities. We demonstrate that, using these discriminant color features, one can achieve results superior to those of the state-of-the-art. Finally, we define an algorithm that allows us to use the learned functional color representation in still images. This is done by learning the mapping between images and the identified functional color features in videos. Our algorithm works in realtime, i.e., > 30 frames/second/CPU thread.
keywords: {emotion recognition;face recognition;feature extraction;image colour analysis;image representation;learning (artificial intelligence);transforms;face areas;image properties;facial action units;AUs;functional color changes;skin color;chromaticity changes;image changes;functional classifiers;discriminant color features;learned functional color representation;identified functional color features;discriminant functional learning;fundamental image feature;facial expressions;blood rushes;Image color analysis;Face recognition;Video sequences;Videos;Transforms;Gabor transforms;Facial expressions of emotion;face recognition;face perception;facial color;compound emotions;Gabor transform;color vision;time invariant;recognition in video;recognition in still images},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8454901&isnumber=8890757

L. Talker, Y. Moses and I. Shimshoni, "Estimating the Number of Correct Matches Using Only Spatial Order," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 2846-2860, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2869560
Abstract: Correctly matching feature points in a pair of images is an important preprocessing step for many computer vision applications. In this paper we propose an efficient method for estimating the number of correct matches without explicitly computing them. To this end, we propose to analyze the set of matches using the spatial order of the features, as projected to the x-axis of the image. The set of features in each image is thus represented by a sequence, and analyzed using the Kendall and Spearman Footrule distance metrics between permutations. This result is interesting in its own right. Moreover, we demonstrate three useful applications of our method: (i) a new halting condition for RANSAC based epipolar geometry estimation methods, (ii) discarding spatially unrelated image pairs in the Structure-from-Motion pipeline, and (iii) computing the probability that a given match is correct based on the rank of the features within the sequences. Our experiments on a large number of synthetic and real data demonstrate the effectiveness of our method. For example, the running time of the image matching stage in the Structure-from-Motion pipeline may be reduced by about 90 percent while preserving about 85 percent of the image pairs with spatial overlap.
keywords: {estimation theory;geometry;image matching;image motion analysis;image representation;image sequences;probability;Kendall Footrule distance metrics;Spearman Footrule distance metrics;image matching stage;structure-from-motion pipeline;spatially unrelated image pairs;RANSAC based epipolar geometry estimation methods;computer vision applications;Pipelines;Computer vision;Estimation;Pattern matching;Measurement;Geometry;Three-dimensional displays;Feature matching;RANSAC;spatial order;correct matches},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8458181&isnumber=8890757

Z. Ding, M. Shao and Y. Fu, "Generative Zero-Shot Learning via Low-Rank Embedded Semantic Dictionary," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 2861-2874, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2867870
Abstract: Zero-shot learning for visual recognition, which approaches identifying unseen categories through a shared visual-semantic function learned on the seen categories and is expected to well adapt to unseen categories, has received considerable research attention most recently. However, the semantic gap between discriminant visual features and their underlying semantics is still the biggest obstacle, because there usually exists domain disparity across the seen and unseen classes. To deal with this challenge, we design two-stage generative adversarial networks to enhance the generalizability of semantic dictionary through low-rank embedding for zero-shot learning. In detail, we formulate a novel framework to simultaneously seek a two-stage generative model and a semantic dictionary to connect visual features with their semantics under a low-rank embedding. Our first-stage generative model is able to augment more semantic features for the unseen classes, which are then used to generate more discriminant visual features in the second stage, to expand the seen visual feature space. Therefore, we will be able to seek a better semantic dictionary to constitute the latent basis for the unseen classes based on the augmented semantic and visual data. Finally, our approach could capture a variety of visual characteristics from seen classes that are “ready-to-use” for new classes. Extensive experiments on four zero-shot benchmarks demonstrate that our proposed algorithm outperforms the state-of-the-art zero-shot algorithms.
keywords: {feature extraction;generalisation (artificial intelligence);image classification;image representation;learning (artificial intelligence);neural nets;generative adversarial networks;zero-shot learning;semantic features;discriminant visual features;augmented semantic data;generative zero-shot learning;low-rank embedded semantic dictionary;visual recognition;zero-shot algorithms;generalizability;image features;visual image representation;Semantics;Visualization;Dictionaries;Generative adversarial networks;Training data;Data models;Generative adversarial network;low-rank embedding;semantic dictionary;zero-shot learning},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8451907&isnumber=8890757

W. A. P. Smith, R. Ramamoorthi and S. Tozza, "Height-from-Polarisation with Unknown Lighting or Albedo," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 2875-2888, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2868065
Abstract: We present a method for estimating surface height directly from a single polarisation image simply by solving a large, sparse system of linear equations. To do so, we show how to express polarisation constraints as equations that are linear in the unknown height. The local ambiguity in the surface normal azimuth angle is resolved globally when the optimal surface height is reconstructed. Our method is applicable to dielectric objects exhibiting diffuse and specular reflectance, though lighting and albedo must be known. We relax this requirement by showing that either spatially varying albedo or illumination can be estimated from the polarisation image alone using nonlinear methods. In the case of illumination, the estimate can only be made up to a binary ambiguity which we show is a generalised Bas-relief transformation corresponding to the convex/concave ambiguity. We believe that our method is the first passive, monocular shape-from-x technique that enables well-posed height estimation with only a single, uncalibrated illumination condition. We present results on real world data, including in uncontrolled, outdoor illumination.
keywords: {image reconstruction;lighting;local ambiguity;surface normal azimuth angle;dielectric objects;specular reflectance;nonlinear methods;binary ambiguity;generalised Bas-relief transformation;passive shape-from-x technique;monocular shape-from-x technique;height estimation;single illumination condition;uncalibrated illumination condition;uncontrolled illumination;outdoor illumination;height-from-polarisation;albedo;single polarisation image;sparse system;linear equations;polarisation constraints;optimal surface height reconstruction;Lighting;Light sources;Refractive index;Surface reconstruction;Solid modeling;Estimation;Polarisation;shape-from-x;bas-relief ambiguity;illumination estimation;albedo estimation},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8456615&isnumber=8890757

S. Lin, R. Ji, C. Chen, D. Tao and J. Luo, "Holistic CNN Compression via Low-Rank Decomposition with Knowledge Transfer," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 2889-2905, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2873305
Abstract: Convolutional neural networks (CNNs) have achieved remarkable success in various computer vision tasks, which are extremely powerful to deal with massive training data by using tens of millions of parameters. However, CNNs often cost significant memory and computation consumption, which prohibits their usage in resource-limited environments such as mobile or embedded devices. To address the above issues, the existing approaches typically focus on either accelerating the convolutional layers or compressing the fully-connected layers separatedly, without pursuing a joint optimum. In this paper, we overcome such a limitation by introducing a holistic CNN compression framework, termed LRDKT, which works throughout both convolutional and fully-connected layers. First, a low-rank decomposition (LRD) scheme is proposed to remove redundancies across both convolutional kernels and fullyconnected matrices, which has a novel closed-form solver to significantly improve the efficiency of the existing iterative optimization solvers. Second, a novel knowledge transfer (KT) based training scheme is introduced. To recover the accumulated accuracy loss and overcome the vanishing gradient, KT explicitly aligns outputs and intermediate responses from a teacher (original) network to its student (compressed) network. We have comprehensively analyzed and evaluated the compression and speedup ratios of the proposed model on MNIST and ILSVRC 2012 benchmarks. In both benchmarks, the proposed scheme has demonstrated superior performance gains over the state-of-the-art methods. We also demonstrate the proposed compression scheme for the task of transfer learning, including domain adaptation and object detection, which show exciting performance gains over the state-of-the-arts. Our source code and compressed models are available at https://github.com/ShaohuiLin/LRDKT.
keywords: {convolutional neural nets;data compression;image coding;image recognition;iterative methods;learning (artificial intelligence);object detection;optimisation;convolutional neural networks;CNN;computer vision;convolutional layers;fully-connected layers;low-rank decomposition scheme;convolutional kernels;knowledge transfer;KT;compression scheme;transfer learning;compressed models;holistic CNN compression;iterative optimization solvers;MNIST;ILSVRC 2012;LRDKT;Knowledge transfer;Image coding;Task analysis;Information exchange;Computational modeling;Convolutional codes;Convolutional neural networks;low-rank decomposition;knowledge transfer;CNN compression;CNN acceleration},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8478366&isnumber=8890757

K. Tanaka, Y. Mukaigawa, T. Funatomi, H. Kubo, Y. Matsushita and Y. Yagi, "Material Classification from Time-of-Flight Distortions," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 2906-2918, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2869885
Abstract: This paper presents a material classification method using an off-the-shelf Time-of-Flight (ToF) camera. The proposed method is built upon a key observation that the depth measurement by a ToF camera is distorted for objects with certain materials, especially with translucent materials. We show that this distortion is due to the variation of time domain impulse responses across materials and also due to the measurement mechanism of the ToF cameras. Specifically, we reveal that the amount of distortion varies according to the modulation frequency of the ToF camera, the object material, and the distance between the camera and object. Our method uses the depth distortion of ToF measurements as a feature for classification and achieves material classification of a scene. Effectiveness of the proposed method is demonstrated by numerical evaluations and real-world experiments, showing its capability of material classification, even for visually indistinguishable objects.
keywords: {cameras;feature extraction;image classification;spatial variables measurement;Time-of-flight distortions;material classification method;off-the-shelf Time-of-Flight camera;depth measurement;ToF camera;translucent materials;time domain impulse responses;measurement mechanism;object material;depth distortion;ToF measurements;Image classification;Distortion measurement;Time-domain analysis;Optical distortion;Optical imaging;Frequency measurement;Alternative sensor;subsurface scattering;time-of-flight camera;temporal point spread functions},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8462767&isnumber=8890757

R. T. Arn, P. Narayana, T. Emerson, B. A. Draper, M. Kirby and C. Peterson, "Motion Segmentation via Generalized Curvatures," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 2919-2932, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2869741
Abstract: New depth sensors, like the Microsoft Kinect, produce streams of human pose data. These discrete pose streams can be viewed as noisy samples of an underlying continuous ideal curve that describes a trajectory through high-dimensional pose space. This paper introduces a technique for generalized curvature analysis (GCA) that determines features along the trajectory which can be used to characterize change and segment motion. Tools are developed for approximating generalized curvatures at mean points along a curve in terms of the singular values of local mean-centered data balls. The features of the GCA algorithm are illustrated on both synthetic and real examples, including data collected from a Kinect II sensor. We also applied GCA to the Carnegie Mellon University Motion Capture (MoCaP) database. Given that GCA scales linearly with the length of the time series we are able to analyze large data sets without down sampling. It is demonstrated that the generalized curvature approximations can be used to segment pose streams into motions and transitions between motions. The GCA algorithm can identify 94.2 percent of the transitions between motions without knowing the set of possible motions in advance, even though the subjects do not stop or pause between motions.
keywords: {approximation theory;image motion analysis;image reconstruction;image segmentation;image sensors;pose estimation;data sets;generalized curvature approximations;GCA algorithm;Motion segmentation;new depth sensors;Microsoft Kinect;human pose data;noisy samples;generalized curvature analysis;mean points;singular values;data balls;Kinect II sensor;Carnegie Mellon University Motion Capture database;GCA scales;continuous ideal curve;Motion segmentation;Computer vision;Approximation algorithms;Trajectory;Sensors;Noise measurement;Generalized curvature analysis;local SVD;motion segmentation;video segmentation},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8462777&isnumber=8890757

X. Zhuang, "Multivariate Mixture Model for Myocardial Segmentation Combining Multi-Source Images," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 2933-2946, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2869576
Abstract: The author proposes a method for simultaneous registration and segmentation of multi-source images, using the multivariate mixture model (MvMM) and maximum of log-likelihood (LL) framework. Specifically, the method is applied to the problem of myocardial segmentation combining the complementary information from multi-sequence (MS) cardiac magnetic resonance (CMR) images. For the image misalignment and incongruent data, the MvMM is formulated with transformations and is further generalized for dealing with the hetero-coverage multi-modality images (HC-MMIs). The segmentation of MvMM is performed in a virtual common space, to which all the images and misaligned slices are simultaneously registered. Furthermore, this common space can be divided into a number of sub-regions, each of which contains congruent data, thus the HC-MMIs can be modeled using a set of conventional MvMMs. Results show that MvMM obtained significantly better performance compared to the conventional approaches and demonstrated good potential for scar quantification as well as myocardial segmentation. The generalized MvMM has also demonstrated better robustness in the incongruent data, where some images may not fully cover the region of interest, and the full coverage can only be reconstructed combining the images from multiple sources.
keywords: {biomedical MRI;cardiology;image reconstruction;image registration;image segmentation;medical image processing;heterocoverage multimodality images;multisource images;HC-MMI;generalized MvMM;hetero-coverage multimodality images;image misalignment;multisequence cardiac magnetic resonance images;log-likelihood framework;simultaneous registration;myocardial segmentation;mixture model;Myocardium;Image segmentation;Mixture models;Biomedical imaging;Pathology;Magnetic resonance;Multivariate image;multi-modality;segmentation;registration;medical image analysis;cardiac MRI},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8458220&isnumber=8890757

X. Mao, Q. Li, H. Xie, R. Y. K. Lau, Z. Wang and S. P. Smolley, "On the Effectiveness of Least Squares Generative Adversarial Networks," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 2947-2960, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2872043
Abstract: Unsupervised learning with generative adversarial networks (GANs) has proven to be hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss for both the discriminator and the generator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson χ2 divergence. We also show that the derived objective function that yields minimizing the Pearson χ2 divergence performs better than the classical one of using least squares for classification. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stably during the learning process. For evaluating the image quality, we conduct both qualitative and quantitative experiments, and the experimental results show that LSGANs can generate higher quality images than regular GANs. Furthermore, we evaluate the stability of LSGANs in two groups. One is to compare between LSGANs and regular GANs without gradient penalty. We conduct three experiments, including Gaussian mixture distribution, difficult architectures, and a newly proposed method - datasets with small variability, to illustrate the stability of LSGANs. The other one is to compare between LSGANs with gradient penalty (LSGANs-GP) and WGANs with gradient penalty (WGANs-GP). The experimental results show that LSGANs-GP succeed in training for all the difficult architectures used in WGANs-GP, including 101-layer ResNet.
keywords: {image classification;learning (artificial intelligence);least squares approximations;learning process;sigmoid cross entropy loss function;least square generative adversarial networks;LSGAN-GP;Pearson χ2 divergence;Gaussian mixture distribution;WGAN-GP;gradient penalty;101-layer ResNet;Generators;Linear programming;Task analysis;Generative adversarial networks;Stability analysis;Least squares GANs; $\chi ^2$ χ 2 divergence;generative model;image generation},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8471208&isnumber=8890757

B. Su and G. Hua, "Order-Preserving Optimal Transport for Distances between Sequences," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 2961-2974, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2870154
Abstract: We present new distance measures between sequences that can tackle local temporal distortion and periodic sequences with arbitrary starting points. Through viewing the instances of each sequence as empirical samples of an unknown distribution, we cast the calculations of distances between sequences as optimal transport problems. To preserve the inherent temporal relationships of the instances in sequences, we propose two methods through incorporating the temporal information into the spatial ground metric and concentrating the transport with two novel temporal regularization terms, respectively. The inverse difference moment regularization enforces local homogeneous structures in the transport, and the KL-divergence with a prior distribution regularization prevents transport between instances with far temporal positions. We show that the resulting problems can be efficiently solved by the matrix scaling algorithm. Extensive experiments on eight datasets with different classifiers and performance measures show the effectiveness and generality of the proposed distances.
keywords: {distance measurement;feature extraction;image classification;image sequences;iterative methods;matrix algebra;optimisation;temporal positions;order-preserving optimal transport;local temporal distortion;periodic sequences;arbitrary starting points;empirical samples;unknown distribution;optimal transport problems;inherent temporal relationships;temporal information;spatial ground metric;temporal regularization terms;inverse difference moment regularization;prior distribution regularization;local homogeneous structures;Solid modeling;Legged locomotion;Distortion measurement;Supervised learning;Optimal transport;sequence matching;order-preserving Wasserstein distance;temporal regularization;inverse difference moment},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8466003&isnumber=8890757

Y. Xu, S. Gao, J. Wu, N. Li and J. Yu, "Personalized Saliency and Its Prediction," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 2975-2989, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2866563
Abstract: Nearly all existing visual saliency models by far have focused on predicting a universal saliency map across all observers. Yet psychology studies suggest that visual attention of different observers can vary significantly under specific circumstances, especially a scene is composed of multiple salient objects. To study such heterogenous visual attention pattern across observers, we first construct a personalized saliency dataset and explore correlations between visual attention, personal preferences, and image contents. Specifically, we propose to decompose a personalized saliency map (referred to as PSM) into a universal saliency map (referred to as USM) predictable by existing saliency detection models and a new discrepancy map across users that characterizes personalized saliency. We then present two solutions towards predicting such discrepancy maps, i.e., a multi-task convolutional neural network (CNN) framework and an extended CNN with Person-specific Information Encoded Filters (CNN-PIEF). Extensive experimental results demonstrate the effectiveness of our models for PSM prediction as well their generalization capabilityfor unseen observers.
keywords: {convolutional neural nets;image processing;prediction theory;psychology;visual saliency models;universal saliency map;psychology studies;heterogenous visual attention pattern;personalized saliency dataset;personal preferences;personalized saliency map;saliency detection models;discrepancy map;multitask convolutional neural network framework;PSM prediction;generalization capability;person-specific information encoded filters;Observers;Saliency detection;Feature extraction;Visualization;Semantics;Predictive models;Image color analysis;Universal saliency;personalized saliency;multi-task learning;convolutional neural network},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8444709&isnumber=8890757

Y. Liu, X. Yuan, J. Suo, D. J. Brady and Q. Dai, "Rank Minimization for Snapshot Compressive Imaging," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 2990-3006, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2873587
Abstract: Snapshot compressive imaging (SCI) refers to compressive imaging systems where multiple frames are mapped into a single measurement, with video compressive imaging and hyperspectral compressive imaging as two representative applications. Though exciting results of high-speed videos and hyperspectral images have been demonstrated, the poor reconstruction quality precludes SCI from wide applications. This paper aims to boost the reconstruction quality of SCI via exploiting the high-dimensional structure in the desired signal. We build a joint model to integrate the nonlocal self-similarity of video/hyperspectral frames and the rank minimization approach with the SCI sensing process. Following this, an alternating minimization algorithm is developed to solve this non-convex problem. We further investigate the special structure of the sampling process in SCI to tackle the computational workload and memory issues in SCI reconstruction. Both simulation and real data (captured by four different SCI cameras) results demonstrate that our proposed algorithm leads to significant improvements compared with current state-of-the-art algorithms. We hope our results will encourage the researchers and engineers to pursue further in compressive imaging for real applications.
keywords: {concave programming;data compression;hyperspectral imaging;image coding;image denoising;image reconstruction;minimisation;video signal processing;compressive imaging systems;video compressive imaging;hyperspectral compressive imaging;high-speed videos;rank minimization;SCI sensing process;alternating minimization algorithm;SCI reconstruction;SCI cameras;snapshot compressive imaging;Image coding;Image reconstruction;Minimization;Hyperspectral imaging;Sensors;Compressive sensing;computational imaging;coded aperture;image processing;video processing;nuclear norm;rank minimization;low rank;hyperspectral images;coded aperture snapshot spectral imaging (CASSI);coded aperture compressive temporal imaging (CACTI)},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8481592&isnumber=8890757

L. Tran, X. Yin and X. Liu, "Representation Learning by Rotating Your Faces," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 3007-3021, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2868350
Abstract: The large pose discrepancy between two face images is one of the fundamental challenges in automatic face recognition. Conventional approaches to pose-invariant face recognition either perform face frontalization on, or learn a pose-invariant representation from, a non-frontal face image. We argue that it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes a Disentangled Representation learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator enables DR-GAN to learn a representation that is both generative and discriminative, which can be used for face image synthesis and pose-invariant face recognition. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one unified identity representation along with an arbitrary number of synthetic face images. Extensive quantitative and qualitative evaluation on a number of controlled and in-the-wild databases demonstrate the superiority of DR-GAN over the state of the art in both learning representations and rotating large-pose face images.
keywords: {face recognition;image representation;learning (artificial intelligence);neural nets;pose estimation;DR-GAN;pose-invariant face recognition;face frontalization;pose-invariant representation;nonfrontal face image;face image synthesis;face variations;pose code;disentangled representation learning-generative adversarial network;Face recognition;Generators;Generative adversarial networks;Image generation;Image quality;Task analysis;Representation learning;generative adversarial network;pose-invariant face recognition;face rotation and frontalization},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8453880&isnumber=8890757

H. Zhou, T. Zhang and J. Jagadeesan, "Re-weighting and 1-Point RANSAC-Based P$n$nP Solution to Handle Outliers," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 3022-3033, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2871832
Abstract: The ability to handle outliers is essential for performing the perspective-n-point (PnP) approach in practical applications, but conventional RANSAC+P3P or P4P methods have high time complexities. We propose a fast PnP solution named R1PPnP to handle outliers by utilizing a soft re-weighting mechanism and the 1-point RANSAC scheme. We first present a PnP algorithm, which serves as the core of R1PPnP, for solving the PnP problem in outlier-free situations. The core algorithm is an optimal process minimizing an objective function conducted with a random control point. Then, to reduce the impact of outliers, we propose a reprojection error-based re-weighting method and integrate it into the core algorithm. Finally, we employ the 1-point RANSAC scheme to try different control points. Experiments with synthetic and real-world data demonstrate that R1PPnP is faster than RANSAC+P3P or P4P methods especially when the percentage of outliers is large, and is accurate. Besides, comparisons with outlier-free synthetic data show that R1PPnP is among the most accurate and fast PnP solutions, which usually serve as the final refinement step of RANSAC+P3P or P4P. Compared with REPPnP, which is the state-of-the-art PnP algorithm with an explicit outliers-handling mechanism, R1PPnP is slower but does not suffer from the percentage of outliers limitation as REPPnP.
keywords: {cameras;computational complexity;computer vision;iterative methods;minimisation;perspective-n-point approach;fast PnP solution named R1PPnP;soft re-weighting mechanism;1-point RANSAC scheme;PnP algorithm;PnP problem;outlier-free situations;core algorithm;random control point;reprojection error-based re-weighting method;explicit outliers-handling mechanism;outlier-free synthetic data;Cameras;Iterative methods;Linear programming;Time complexity;Three-dimensional displays;Pose estimation;Perspective- $n$ n -point, 1-point RANSAC, soft re-weighting;robustness to outliers},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8470970&isnumber=8890757

A. Dutta, J. Engels and M. Hahn, "Segmentation of Laser Point Clouds in Urban Areas by a Modified Normalized Cut Method," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 3034-3047, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2869744
Abstract: Normalized Cut is a well-established divisive image segmentation method, which we adapt in this paper for the segmentation of laser point clouds in urban areas. Our focus is on polyhedral objects with planar surfaces. Due to its target function, Normalized Cut favours cuts with “short cut lines” or “small cut surfaces”, which is a drawback for our application. We therefore modify the target function, weighting the similarity measures with distance-dependent weights. We call the induced minimization problem “Distance-weighted Cut” (DWCut). The new target function leads to a generalized eigenvalue problem, which is slightly more complicated than the corresponding problem for the Normalized Cut; on the other hand, the new target function is easier to interpret and avoids some drawbacks of the Normalized Cut. We point out an efficient method for the numerical solution of the eigenvalue problem which is based on a Krylov subspace method. DWCut can be beneficially combined with an aggregation in order to reduce the computational effort and to avoid shortcomings due to insufficient plane parameters. We present examples for the successful application of the Distance-weighted Cut principle and evaluate its results by comparison with the results of corresponding manual segmentations.
keywords: {computational geometry;eigenvalues and eigenfunctions;image segmentation;minimisation;target function;Distance-weighted Cut principle;laser point cloud segmentation;urban areas;modified Normalized Cut method;divisive image segmentation method;short cut lines;small cut surfaces;induced minimization problem;Krylov subspace method;Image segmentation;Three-dimensional displays;Laser beam cutting;Cost function;Urban areas;Minimization;Eigenvalues and eigenfunctions;Graph;cut;segmentation;laser point cloud},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8462771&isnumber=8890757

S. Chen and Q. Zhao, "Shallowing Deep Networks: Layer-Wise Pruning Based on Feature Representations," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 3048-3056, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2874634
Abstract: Recent surge of Convolutional Neural Networks (CNNs) has brought successes among various applications. However, these successes are accompanied by a significant increase in computational cost and the demand for computational resources, which critically hampers the utilization of complex CNNs on devices with limited computational power. In this work, we propose a feature representation based layer-wise pruning method that aims at reducing complex CNNs to more compact ones with equivalent performance. Different from previous parameter pruning methods that conduct connection-wise or filter-wise pruning based on weight information, our method determines redundant parameters by investigating the features learned in the convolutional layers and the pruning process is operated at a layer level. Experiments demonstrate that the proposed method is able to significantly reduce computational cost and the pruned models achieve equivalent or even better performance compared to the original models on various datasets.
keywords: {convolutional neural nets;learning (artificial intelligence);computational resources;complex CNNs;computational power;feature representation based layer-wise pruning method;filter-wise pruning;convolutional layers;pruning process;computational cost;deep Networks;feature representations;convolutional neural networks;connection-wise pruning;parameter pruning methods;Computational modeling;Computational efficiency;Feature extraction;Task analysis;Convolutional neural networks;Acceleration;Model pruning;compact design;convolutional neural networks},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8485719&isnumber=8890757

M. Jaberi, M. Pensky and H. Foroosh, "Sparse One-Grab Sampling with Probabilistic Guarantees," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 3057-3070, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2871850
Abstract: Sampling is an important and effective strategy in analyzing “big data,” whereby a smaller subset of a dataset is used to estimate the characteristics of its entire population. The main goal in sampling is often to achieve a significant gain in the computational time. However, a major obstacle towards this goal is the assessment of the smallest sample size needed to ensure, with a high probability, a faithful representation of the entire dataset, especially when the data set is compiled of a large number of diverse structures (e.g., clusters). To address this problem, we propose a method referred to as the Sparse Withdrawal of Inliers in a First Trial (SWIFT) that determines the smallest sample size of a subset of a dataset sampled in one grab, with the guarantee that the subset provides a sufficient number of samples from each of the underlying structures necessary for the discovery and inference. The latter is established with high probability, and the lower bound of the smallest sample size depends on probabilistic guarantees. In addition, we derive an upper bound on the smallest sample size that allows for detection of the structures and show that the two bounds are very close to each other in a variety of scenarios. We show that the problem can be modeled using either a hypergeometric or a multinomial probability mass function (pmf), and derive accurate mathematical bounds to determine a tight approximation to the sample size, leading thus to a sparse sampling strategy. The key features of the proposed method are: (i) sparseness of the sampled subset for analyzing data, where the level of sparseness is independent of the population size; (ii) no prior knowledge of the distribution of data, or the number of underlying structures in the data; and (iii) robustness in the presence of overwhelming number of outliers. We evaluate the method thoroughly in terms of accuracy, its behavior against different parameters, and its effectiveness in reducing the computational cost in various applications of computer vision, such as subspace clustering and structure from motion.
keywords: {approximation theory;Big Data;data analysis;data structures;sampling methods;smallest sample size;probabilistic guarantees;sparse sampling strategy;sampled subset;Sparse one-grab sampling;data set;Data models;Sociology;Computational modeling;Mathematical model;Sampling methods;Iterative methods;Sampling big data;sample size;probabilistic guarantees;parameter/structure estimation;subspace clustering},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8470977&isnumber=8890757

M. Long, Y. Cao, Z. Cao, J. Wang and M. I. Jordan, "Transferable Representation Learning with Deep Adaptation Networks," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 3071-3085, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2868685
Abstract: Domain adaptation studies learning algorithms that generalize across source domains and target domains that exhibit different distributions. Recent studies reveal that deep neural networks can learn transferable features that generalize well to similar novel tasks. However, as deep features eventually transition from general to specific along the network, feature transferability drops significantly in higher task-specific layers with increasing domain discrepancy. To formally reduce the effects of this discrepancy and enhance feature transferability in task-specific layers, we develop a novel framework for deep adaptation networks that extends deep convolutional neural networks to domain adaptation problems. The framework embeds the deep features of all task-specific layers into reproducing kernel Hilbert spaces (RKHSs) and optimally matches different domain distributions. The deep features are made more transferable by exploiting low-density separation of target-unlabeled data in very deep architectures, while the domain discrepancy is further reduced via the use of multiple kernel learning that enhances the statistical power of kernel embedding matching. The overall framework is cast in a minimax game setting. Extensive empirical evidence shows that the proposed networks yield state-of-the-art results on standard visual domain-adaptation benchmarks.
keywords: {convolutional neural nets;feature extraction;game theory;generalisation (artificial intelligence);Hilbert spaces;learning (artificial intelligence);minimax techniques;deep features;feature transferability;domain discrepancy;deep adaptation networks;deep convolutional neural networks;domain distributions;deep architectures;multiple kernel learning;transferable representation learning;deep neural networks;transferable feature learning;visual domain-adaptation;learning algorithms;generalization;reproducing kernel Hilbert spaces;kernel embedding matching;minimax game;Task analysis;Learning systems;Adaptation models;Convolutional neural networks;Deep learning;Domain adaptation;deep learning;convolutional neural network;two-sample test;multiple kernel learning},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8454781&isnumber=8890757

Z. Gao, L. Wang, N. Jojic, Z. Niu, N. Zheng and G. Hua, "Video Imprint," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 3086-3099, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2866114
Abstract: A new unified video analytics framework (ER3) is proposed for complex event retrieval, recognition and recounting, based on the proposed video imprint representation, which exploits temporal correlations among image features across video frames. With the video imprint representation, it is convenient to reverse map back to both temporal and spatial locations in video frames, allowing for both key frame identification and key areas localization within each frame. In the proposed framework, a dedicated feature alignment module is incorporated for redundancy removal across frames to produce the tensor representation, i.e., the video imprint. Subsequently, the video imprint is individually fed into both a reasoning network and a feature aggregation module, for event recognition/recounting and event retrieval tasks, respectively. Thanks to its attention mechanism inspired by the memory networks used in language modeling, the proposed reasoning network is capable of simultaneous event category recognition and localization of the key pieces of evidence for event recounting. In addition, the latent structure in our reasoning network highlights the areas of the video imprint, which can be directly used for event recounting. With the event retrieval task, the compact video representation aggregated from the video imprint contributes to better retrieval results than existing state-of-the-art methods.
keywords: {feature extraction;image representation;video retrieval;video signal processing;unified video analytics framework;video imprint representation;video frames;key frame identification;reasoning network;event recognition;event retrieval tasks;event recounting;compact video representation;dedicated feature alignment module;redundancy removal;tensor representation;memory networks;language modeling;simultaneous event category recognition;Task analysis;Feature extraction;Cognition;Computational modeling;Correlation;Neural networks;Layout;Event videos;feature alignment;feature aggregation;reasoning network},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8440050&isnumber=8890757

R. Santa Cruz, B. Fernando, A. Cherian and S. Gould, "Visual Permutation Learning," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 3100-3114, 1 Dec. 2019.
doi: 10.1109/TPAMI.2018.2873701
Abstract: We present a principled approach to uncover the structure of visual data by solving a deep learning task coined visual permutation learning. The goal of this task is to find the permutation that recovers the structure of data from shuffled versions of it. In the case of natural images, this task boils down to recovering the original image from patches shuffled by an unknown permutation matrix. Permutation matrices are discrete, thereby posing difficulties for gradient-based optimization methods. To this end, we resort to a continuous approximation using doubly-stochastic matrices and formulate a novel bi-level optimization problem on such matrices that learns to recover the permutation. Unfortunately, such a scheme leads to expensive gradient computations. We circumvent this issue by further proposing a computationally cheap scheme for generating doubly stochastic matrices based on Sinkhorn iterations. To implement our approach we propose DeepPermNet, an end-to-end CNN model for this task. The utility of DeepPermNet is demonstrated on three challenging computer vision problems, namely, relative attributes learning, supervised learning-to-rank, and self-supervised representation learning. Our results show state-of-the-art performance on the Public Figures and OSR benchmarks for relative attributes learning, chronological and interestingness image ranking for supervised learning-to-rank, and competitive results in the classification and segmentation tasks of the PASCAL VOC dataset for self-supervised representation learning.
keywords: {computer vision;convolutional neural nets;data structures;gradient methods;image classification;image representation;image segmentation;matrix algebra;optimisation;stochastic processes;supervised learning;classification tasks;segmentation tasks;self-supervised representation learning;visual permutation learning;visual data structure;deep learning;natural images;permutation matrix;gradient-based optimization;bilevel optimization problem;doubly stochastic matrices;end-to-end CNN model;computer vision;supervised learning-to-rank;Sinkhorn iterations;DeepPermNet;image representations;Visualization;Task analysis;Machine learning;Image sequences;Computational modeling;Computer vision;Predictive models;Permutation learning;self-supervised learning;relative attributes;representation learning;learning-to-rank},
URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8481554&isnumber=8890757

